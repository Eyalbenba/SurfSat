{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDj7b3dLAL9W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pykml"
      ],
      "metadata": {
        "id": "3maarQrIT56s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Optimal Params for pictures: zoom = 16 , size = \"1280x1280\""
      ],
      "metadata": {
        "id": "MwF71_IqdJVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Intalization"
      ],
      "metadata": {
        "id": "kpm4YdRxL_fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "# from pykml import parser\n",
        "from lxml import etree\n",
        "import os\n",
        "from lxml.etree import ParseError\n",
        "from geopy.geocoders import Nominatim\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "G3IyuYBe_vXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLr0af7UBUW8",
        "outputId": "622e9013-9bbd-4467-b108-85841f238d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "def unzip_images(zip_path, extract_to):\n",
        "    # Check if the extraction directory exists, create if not\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    # Unzip the archive\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    print(f\"Images extracted to {extract_to}\")\n"
      ],
      "metadata": {
        "id": "luhbYCOeBFWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse KML File and Create intial Dataset"
      ],
      "metadata": {
        "id": "96y8AjRrMA_7"
      }
    },
    {
      "metadata": {
        "id": "Efw9YrmZ_giD"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        " #%%writefile create_dataset.py\n",
        "\n",
        "def get_map_snapshot(api_key, coordinates, zoom=16, size=\"1280x1280\", map_type=\"satellite\"):\n",
        "    \"\"\"\n",
        "    Function to get a map snapshot from Google Maps Static API.\n",
        "\n",
        "    :param api_key: Your Google Maps Static API key.\n",
        "    :param coordinates: A tuple or list of the latitude and longitude (e.g., (39.6026, -9.0708)).\n",
        "    :param zoom: Zoom level of the map. Default is 12.\n",
        "    :param size: Size of the map image. Default is 600x300.\n",
        "    :param map_type: Type of the map (roadmap, satellite, hybrid, terrain). Default is satellite.\n",
        "    :return: The response content (image bytes).\n",
        "    \"\"\"\n",
        "    base_url = \"https://maps.googleapis.com/maps/api/staticmap?\"\n",
        "\n",
        "    params = {\n",
        "        \"center\": f\"{coordinates[0]},{coordinates[1]}\",\n",
        "        \"zoom\": zoom,\n",
        "        \"size\": size,\n",
        "        \"maptype\": map_type,\n",
        "        \"key\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        raise Exception(f\"Error fetching map: {response.status_code} - {response.text}\")\n",
        "\n",
        "def save_image(image_content, directory, filename):\n",
        "    \"\"\"\n",
        "    Function to save the image content to a file.\n",
        "\n",
        "    :param image_content: The image content (bytes).\n",
        "    :param directory: The directory to save the image.\n",
        "    :param filename: The filename to save the image.\n",
        "    \"\"\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    filepath = os.path.join(directory, filename)\n",
        "    with open(filepath, \"wb\") as file:\n",
        "        file.write(image_content)\n",
        "    print(f\"Saved image to: {filepath}\")\n",
        "    return os.path.basename(filepath)\n",
        "\n",
        "def get_location_details(latitude, longitude):\n",
        "    geolocator = Nominatim(user_agent=\"myGeoAPI\")\n",
        "    location = geolocator.reverse((latitude, longitude), exactly_one=True)\n",
        "    return location.address if location else \"Location details not found\"\n",
        "\n",
        "def parse_kml(file_path):\n",
        "    \"\"\"\n",
        "    Function to parse a KML file using the pykml parser and extract coordinates, folder names, and placemark names.\n",
        "\n",
        "    :param file_path: Path to the KML file.\n",
        "    :return: List of tuples, each containing coordinates in decimal degrees, folder name, and placemark name.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"KML file not found: {file_path}\")\n",
        "\n",
        "    details_list = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            doc = parser.parse(file).getroot()\n",
        "\n",
        "        # Iterate over all folders in the document\n",
        "        for folder in doc.Document.findall('.//{http://www.opengis.net/kml/2.2}Folder'):\n",
        "            folder_name = folder.name.text if hasattr(folder, 'name') else \"Unnamed Folder\"\n",
        "\n",
        "            # Iterate over all placemarks in the folder\n",
        "            for placemark in folder.findall('.//{http://www.opengis.net/kml/2.2}Placemark'):\n",
        "                placemark_name = placemark.name.text if hasattr(placemark, 'name') else \"Unnamed Placemark\"\n",
        "\n",
        "                # Iterate over all coordinates in the placemark\n",
        "                for coords in placemark.findall('.//{http://www.opengis.net/kml/2.2}coordinates'):\n",
        "                    coord_text = coords.text.strip()\n",
        "                    coord_parts = coord_text.split(',')\n",
        "                    if len(coord_parts) >= 2:\n",
        "                        lon, lat = coord_parts[0], coord_parts[1]\n",
        "                        try:\n",
        "                            loc_details = get_location_details(float(lat), float(lon))\n",
        "                            details_list.append([(float(lat), float(lon)) , folder_name, placemark_name,loc_details])\n",
        "                        except ValueError:\n",
        "                            print(f\"Skipping invalid coordinates: {coord_text}\")\n",
        "            #     continue_user = input('Continue?')\n",
        "            #     if continue_user == 'No':\n",
        "            #       break\n",
        "            # break\n",
        "    except ParseError as e:\n",
        "        print(f\"Error parsing the KML file: {e}\")\n",
        "\n",
        "    return details_list\n",
        "\n",
        "def process_kml_coordinates(api_key, kml_file_path, save_directory):\n",
        "    \"\"\"\n",
        "    Function to process a KML file, fetch map snapshots, and save them with running numbers.\n",
        "\n",
        "    :param api_key: Your Google Maps Static API key.\n",
        "    :param kml_file_path: Path to the KML file.\n",
        "    :param save_directory: Directory to save the images.\n",
        "    \"\"\"\n",
        "    details_list = parse_kml(kml_file_path)\n",
        "\n",
        "    for i, coords in enumerate(details_list, start=1):\n",
        "        try:\n",
        "            image_content = get_map_snapshot(api_key, coords[0])\n",
        "            save_image(image_content, save_directory, f\"map_snapshot_{i}.png\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error with coordinate {coords}: {e}\")\n",
        "\n",
        "def extract_country(location):\n",
        "    search_res = re.search(r'[\\w\\s]+$', location)\n",
        "    if search_res:\n",
        "      if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':\n",
        "        country = 'Marraco'\n",
        "        return country\n",
        "      elif search_res.group().strip() == 'ישראל':\n",
        "        country = 'Israel'\n",
        "        return country\n",
        "      else:\n",
        "        return search_res.group().strip()\n",
        "    else:\n",
        "      return None  # If no country is found, return None\n",
        "\n",
        "def add_terrain(country):\n",
        "  terrain_dict = {'Sri Lanka':'Tropical', 'Maldives': 'Tropical' ,\\\n",
        "                  'El Salvador': 'Tropical', 'Costa Rica': 'Tropical',\\\n",
        "                  'Ecuador': 'Tropical','Marraco': 'Desert','Panamá':'Tropical',\\\n",
        "                  'Indonesia':'Tropical','Nicaragua':'Tropical'}\n",
        "  return terrain_dict.get(country, 'Unknown')\n",
        "\n",
        "\n",
        "def add_image_path(details_df):\n",
        "  api_key = 'YOUR GOOGLE MAPS API KEY HERE'\n",
        "  for index, row in details_df.iterrows():\n",
        "    placemark = row['placemark_num']\n",
        "    coordinates = row['coordinates']\n",
        "    image_content = get_map_snapshot(api_key, coordinates)\n",
        "    image_file_path = save_image(image_content, save_directory, f\"map_snapshot_{placemark}.png\")\n",
        "    details_df.at[index, 'image_path'] = image_file_path  # Ensure the path is correctly assigned back to the DataFrame\n",
        "  return details_df\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Absolute path to the KML file\n",
        "#     kml_file_path = r\"/content/Satalite Surfing Images.kml\"\n",
        "\n",
        "#     # Absolute path to the directory where you want to save the images\n",
        "#     save_directory = r\"/content/Sat_Images\"\n",
        "\n",
        "#     # Your Google Maps Static API key\n",
        "#     api_key = \"YOUR_API_KEY_HERE\"\n",
        "      # data_without_photos = parse_kml(kml_file_path)\n",
        "      # details_df = pd.DataFrame(data_without_photos, columns=['coordinates', 'tagger', 'placemark_num', 'location'])\n",
        "      # details_df['country'] = details_df['location'].apply(extract_country)\n",
        "      # details_df['terrain'] = details_df['country'].apply(add_terrain)\n",
        "      # details_df_with_images = add_image_path(details_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "3JifBl1aLJnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "cUfh6seCL7pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_country(location):\n",
        "    search_res = re.search(r'[\\w\\s]+$', location)\n",
        "    if search_res:\n",
        "      if search_res.group().strip() == 'Maroc ⵍⵎⵖⵔⵉⴱ المغرب':\n",
        "        country = 'Marraco'\n",
        "        return country\n",
        "      elif search_res.group().strip() == 'ישראל':\n",
        "        country = 'Israel'\n",
        "        return country\n",
        "      else:\n",
        "        return search_res.group().strip()\n",
        "    else:\n",
        "      return None  # If no country is found, return None\n",
        "\n",
        "def add_terrain(country):\n",
        "  terrain_dict = {'Sri Lanka':'Tropical', 'Maldives': 'Tropical' ,\\\n",
        "                  'El Salvador': 'Tropical', 'Costa Rica': 'Tropical',\\\n",
        "                  'Ecuador': 'Tropical','Marraco': 'Desert','Panamá':'Tropical',\\\n",
        "                  'Indonesia':'Tropical','Nicaragua':'Tropical'}\n",
        "  return terrain_dict.get(country, 'Unknown')\n",
        "\n",
        "\n",
        "def add_image_path(details_df):\n",
        "  api_key = 'YOUR API KEY HERE'\n",
        "  for index, row in details_df.iterrows():\n",
        "    placemark = row['placemark_num']\n",
        "    coordinates = row['coordinates']\n",
        "    image_content = get_map_snapshot(api_key, coordinates)\n",
        "    image_file_path = save_image(image_content, save_directory, f\"map_snapshot_{placemark}.png\")\n",
        "    details_df.at[index, 'image_path'] = image_file_path  # Ensure the path is correctly assigned back to the DataFrame\n",
        "  return details_df"
      ],
      "metadata": {
        "id": "Mv8gjst3LLzb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the dataset - Main Cell"
      ],
      "metadata": {
        "id": "Co5q9ougMYGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_photos = parse_kml(kml_file_path)\n",
        "details_df = pd.DataFrame(data_without_photos, columns=['coordinates', 'tagger', 'placemark_num', 'location'])\n",
        "details_df['country'] = details_df['location'].apply(extract_country)\n",
        "details_df['terrain'] = details_df['country'].apply(add_terrain)\n",
        "details_df_with_images = add_image_path(details_df)\n",
        "# details_df_with_images.to_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_Image_cleaned_dataset.csv')"
      ],
      "metadata": {
        "id": "sDFDnUfMBRum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unzip Photos"
      ],
      "metadata": {
        "id": "TxAUSsBcL18C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "def zip_directory(folder_path, output_zip):\n",
        "    \"\"\"Zip the contents of an entire directory.\"\"\"\n",
        "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                zipf.write(file_path, os.path.relpath(file_path, os.path.join(folder_path, '..')))\n",
        "\n",
        "# Specify the directory you want to download\n",
        "folder_path = '/content/Sat_Images'  # Adjust this to your folder path\n",
        "output_zip = '/content/Sat_Images.zip'  # Name for the output ZIP file\n",
        "\n",
        "# Create a ZIP file\n",
        "zip_directory(folder_path, output_zip)\n",
        "\n",
        "# Download the ZIP file\n",
        "files.download(output_zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "WWiMmNRPQQPX",
        "outputId": "f152f8fa-9735-48d9-c15f-79f2c57f0eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_610961e6-7a64-4fd9-a3ff-a1106c71e0e0\", \"Sat_Images.zip\", 94892087)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Orginazing\n",
        "\n",
        "This Stage was Executed after manual reviewal of the tagged data"
      ],
      "metadata": {
        "id": "bM59rpMvTYMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0rgA5G_TjVD",
        "outputId": "84a603ea-919f-4eb5-ec67-0e274334a548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_cleaned = pd.read_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_Image_cleaned_dataset.csv')"
      ],
      "metadata": {
        "id": "Zui0m66jV8rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Creating Final Cleaned_df"
      ],
      "metadata": {
        "id": "fot3Rwl_W2l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Cleaning Irrelvant columns\n",
        "full_data_df = pd.read_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_image_data.csv')\n",
        "full_data_df = full_data_df.drop(columns=['Unnamed: 0','Unnamed: 10'])"
      ],
      "metadata": {
        "id": "DrwBCje0UWcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting and flattening the 'extra_features' into a list of unique features\n",
        "full_data_df['extra_features_list'] = full_data_df['extra features'].dropna().apply(lambda x: x.split(' + '))\n",
        "\n",
        "# Flatten the list and get unique features\n",
        "all_features = set()\n",
        "full_data_df['extra_features_list'].dropna().apply(lambda features: [all_features.add(feature) for feature in features])\n",
        "\n",
        "# Create columns for each feature and fill with binary values (0 or 1)\n",
        "for feature in all_features:\n",
        "    full_data_df[feature] = full_data_df['extra features'].apply(lambda x: 1 if isinstance(x, str) and feature in x else 0)\n",
        "\n",
        "print(full_data_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfabwRZPZmRF",
        "outputId": "3ef65d0d-dd0a-46f9-dd0c-9212cdcd217b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 coordinates tagger placemark_num  \\\n",
            "0    (32.22855629832505, -116.9234467023862)  Gilad     Gilad_001   \n",
            "1    (32.20238866696115, -116.9139428948928)  Gilad     Gilad_002   \n",
            "2    (32.18326958506614, -116.9099017074971)  Gilad     Gilad_003   \n",
            "3    (30.54467815330961, -9.726458682969735)   Alon        alon 1   \n",
            "4     (29.88665941651898, -9.78552918565589)   Alon       alon 10   \n",
            "..                                       ...    ...           ...   \n",
            "386  (49.10637675770386, -125.8767321241412)    Ron        Ron 60   \n",
            "387  (49.10891761273925, -125.8811443808606)    Ron        Ron 61   \n",
            "388  (-35.95255978275283, 150.1582975453066)    Ron         Ron 7   \n",
            "389  (-35.86753610565759, 150.1635278545278)    Ron         Ron 8   \n",
            "390  (-35.81141215306061, 150.2261763007545)    Ron         Ron 9   \n",
            "\n",
            "                                              location    country  terrain  \\\n",
            "0    Caseta de vigilancia, Calle Mar De Los Erisos,...     México  Unknown   \n",
            "1    Mision de San Diego, Mision Viejo, Primo Tapia...     México  Unknown   \n",
            "2    Calle Chapala, Municipio de Playas de Rosarito...     México  Unknown   \n",
            "3    Anchor Point, RN1, Taghazout ⵜⴰⵖⴰⵣⵓⵜ تغازوت, c...    Marraco   Desert   \n",
            "4                                  Maroc ⵍⵎⵖⵔⵉⴱ المغرب    Marraco   Desert   \n",
            "..                                                 ...        ...      ...   \n",
            "386  Sunset Point/Pettinger Point trail, Tofino, Al...     Canada  Unknown   \n",
            "387  Sunset Point/Pettinger Point trail, Tofino, Al...     Canada  Unknown   \n",
            "388  East, Point Parade, Congo, Eurobodalla Shire C...  Australia  Unknown   \n",
            "389  Broulee, Eurobodalla Shire Council, New South ...  Australia  Unknown   \n",
            "390  Yowani Road, Eurobodalla Shire Council, New So...  Australia  Unknown   \n",
            "\n",
            "                      image_path                   extra features  Reviewed  \\\n",
            "0    map_snapshot_ Gilad_001.png                              NaN         1   \n",
            "1    map_snapshot_ Gilad_002.png                              NaN         1   \n",
            "2    map_snapshot_ Gilad_003.png                              NaN         1   \n",
            "3        map_snapshot_alon 1.png               Wave Pattern + Bay         1   \n",
            "4       map_snapshot_alon 10.png                              NaN         1   \n",
            "..                           ...                              ...       ...   \n",
            "386      map_snapshot_Ron 60.png                              NaN         1   \n",
            "387      map_snapshot_Ron 61.png                              NaN         1   \n",
            "388       map_snapshot_Ron 7.png  Wave Pattern + Bay + Rivermouth         1   \n",
            "389       map_snapshot_Ron 8.png                              NaN         1   \n",
            "390       map_snapshot_Ron 9.png                              NaN         1   \n",
            "\n",
            "                 extra_features_list  Rivermouth  Bay  Wave Pattern  \n",
            "0                                NaN           0    0             0  \n",
            "1                                NaN           0    0             0  \n",
            "2                                NaN           0    0             0  \n",
            "3                [Wave Pattern, Bay]           0    1             1  \n",
            "4                                NaN           0    0             0  \n",
            "..                               ...         ...  ...           ...  \n",
            "386                              NaN           0    0             0  \n",
            "387                              NaN           0    0             0  \n",
            "388  [Wave Pattern, Bay, Rivermouth]           1    1             1  \n",
            "389                              NaN           0    0             0  \n",
            "390                              NaN           0    0             0  \n",
            "\n",
            "[391 rows x 13 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_df = full_data_df.drop(columns=['extra features'])"
      ],
      "metadata": {
        "id": "MYlhJfuiaCFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_data_df.to_csv('/content/drive/MyDrive/Satalite_Surf_Project/Satalite_Surf_Image_cleaned_dataset.csv')"
      ],
      "metadata": {
        "id": "pUL7bpcFaK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get DataFrame by Feature"
      ],
      "metadata": {
        "id": "2cD0QKylXpbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_df_by_feature(df):\n",
        "    # Positive examples\n",
        "    bay_df = df[df['Bay'] == 1]\n",
        "    rivermouth_df = df[df['Rivermouth'] == 1]\n",
        "    wavepattern_df = df[df['Wave Pattern'] == 1]\n",
        "\n",
        "    # Determine the number of positive examples\n",
        "    num_bay = len(bay_df)\n",
        "    num_rivermouth = len(rivermouth_df)\n",
        "    num_wavepattern = len(wavepattern_df)\n",
        "\n",
        "    # Negative examples, matched in number to positive examples\n",
        "    non_bay_df = df[df['Bay'] == 0].sample(min(num_bay, len(df[df['Bay'] == 0])), random_state=42)\n",
        "    non_rivermouth_df = df[df['Rivermouth'] == 0].sample(min(num_rivermouth, len(df[df['Rivermouth'] == 0])), random_state=42)\n",
        "    non_wavepattern_df = df[df['Wave Pattern'] == 0].sample(min(num_wavepattern, len(df[df['Wave Pattern'] == 0])), random_state=42)\n",
        "\n",
        "    # Append negative examples to each DataFrame\n",
        "    bay_df = pd.concat([bay_df, non_bay_df]).sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    rivermouth_df = pd.concat([rivermouth_df, non_rivermouth_df]).sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "    wavepattern_df = pd.concat([wavepattern_df, non_wavepattern_df]).sample(frac=1).reset_index(drop=True)  # Shuffle\n",
        "\n",
        "    # DataFrame containing examples that do not belong to any specific class\n",
        "    other_df = df[(df['Wave Pattern'] != 1) & (df['Rivermouth'] != 1) & (df['Bay'] != 1)]\n",
        "\n",
        "    return bay_df, rivermouth_df, wavepattern_df, other_df\n",
        "\n",
        "def get_balanced_onlygood_df(df):\n",
        "    # Define positive examples: having any of the features\n",
        "    onlygood_df = df[(df['Wave Pattern'] == 1) | (df['Rivermouth'] == 1) | (df['Bay'] == 1)]\n",
        "\n",
        "    # Define negative examples: having none of the features\n",
        "    non_onlygood_df = df[(df['Wave Pattern'] != 1) & (df['Rivermouth'] != 1) & (df['Bay'] != 1)]\n",
        "\n",
        "    # Calculate the minimum number of examples to make the dataset balanced\n",
        "    min_count = min(len(onlygood_df), len(non_onlygood_df))\n",
        "\n",
        "    # Sample from both positive and negative examples to make them balanced\n",
        "    onlygood_df_balanced = onlygood_df.sample(min_count, random_state=42)\n",
        "    non_onlygood_df_balanced = non_onlygood_df.sample(min_count, random_state=42)\n",
        "\n",
        "    # Combine and shuffle the balanced dataset\n",
        "    balanced_df = pd.concat([onlygood_df_balanced, non_onlygood_df_balanced]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    return balanced_df\n"
      ],
      "metadata": {
        "id": "2kpzasVJXNfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# balanced_df = get_balanced_onlygood_df(data_cleaned)\n",
        "# balanced_df['Good'] = ((balanced_df['Wave Pattern'] == 1) | (balanced_df['Rivermouth'] == 1) | (balanced_df['Bay'] == 1)).astype(int)\n"
      ],
      "metadata": {
        "id": "UpjNV6kOfbAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_and_sum_features(df):\n",
        "    # Count the occurrences of each feature and the absence of all\n",
        "    is_bay = (df['Bay'] == 1)\n",
        "    is_rivermouth = (df['Rivermouth'] == 1)\n",
        "    is_wavepattern = (df['Wave Pattern'] == 1)\n",
        "    is_none = (~is_bay & ~is_rivermouth & ~is_wavepattern)  # None of the features are present\n",
        "\n",
        "    # Summing the True values to get the count of rows meeting the conditions\n",
        "    total_bay = is_bay.sum()\n",
        "    total_rivermouth = is_rivermouth.sum()\n",
        "    total_wavepattern = is_wavepattern.sum()\n",
        "    total_none = is_none.sum()\n",
        "\n",
        "    # Sum of instances where any of Bay, Rivermouth, or Wave Pattern is present\n",
        "    total_feature_present = total_bay + total_rivermouth + total_wavepattern\n",
        "\n",
        "    print(f\"Total 'Bay' instances: {total_bay}\")\n",
        "    print(f\"Total 'Rivermouth' instances: {total_rivermouth}\")\n",
        "    print(f\"Total 'Wave Pattern' instances: {total_wavepattern}\")\n",
        "    print(f\"Total 'None' instances: {total_none}\")\n",
        "    print(f\"Total instances with any feature (Bay, Rivermouth, Wave Pattern): {total_feature_present}\")\n",
        "    print(f\"Total instances with none of the features: {total_none}\")\n",
        "\n",
        "# Example usage, assuming balanced_onlygood_df is your DataFrame\n",
        "# count_and_sum_features(balanced_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6YRcv9Cfd5m",
        "outputId": "36dfa561-2675-4ebf-c26f-a53b3d3e4059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 'Bay' instances: 82\n",
            "Total 'Rivermouth' instances: 63\n",
            "Total 'Wave Pattern' instances: 89\n",
            "Total 'None' instances: 170\n",
            "Total instances with any feature (Bay, Rivermouth, Wave Pattern): 234\n",
            "Total instances with none of the features: 170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bay_df,rivermouth_df,wavepattern_df,other_df = get_df_by_feature(data_cleaned)\n",
        "print(bay_df['Bay'].value_counts())\n",
        "print(rivermouth_df['Rivermouth'].value_counts())\n",
        "print(wavepattern_df['Wave Pattern'].value_counts())"
      ],
      "metadata": {
        "id": "nYCXgGkOX5S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(bay_df['Bay'].value_counts())\n",
        "# print(rivermouth_df['Rivermouth'].value_counts())\n",
        "# print(wavepattern_df['Wave Pattern'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClRsIJpdtO2A",
        "outputId": "2a947d6c-0113-48f0-8036-77f1cc229f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bay\n",
            "0    82\n",
            "1    82\n",
            "Name: count, dtype: int64\n",
            "Rivermouth\n",
            "1    63\n",
            "0    63\n",
            "Name: count, dtype: int64\n",
            "Wave Pattern\n",
            "1    89\n",
            "0    89\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Uploading"
      ],
      "metadata": {
        "id": "eDLuX4bWTcQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "\n",
        "def split_and_save_data(df, output_dir, stratify_by_cols, target_label):\n",
        "    # Extend the stratification column to include the target label\n",
        "    df['stratify_col'] = df[stratify_by_cols[0]] + \"_\" + df[stratify_by_cols[1]] + \"_\" + df[target_label].astype(str)\n",
        "    counts = df['stratify_col'].value_counts()\n",
        "    small_categories = counts[counts <5].index\n",
        "    print(counts)\n",
        "    # Assign a new category for small groups\n",
        "    df['stratify_col_adjusted'] = df['stratify_col'].apply(lambda x: 'Other' if x in small_categories else x)\n",
        "    adjusted_counts = df['stratify_col_adjusted'].value_counts()\n",
        "    print(adjusted_counts)\n",
        "    # Initialize the StratifiedShuffleSplit object\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)  # Split for test set 10%\n",
        "\n",
        "    # Performing the first split to separate out the test set\n",
        "    for train_val_index, test_index in sss.split(df, df['stratify_col_adjusted']):\n",
        "        train_val_set = df.iloc[train_val_index]\n",
        "        test_set = df.iloc[test_index]\n",
        "\n",
        "    # Adjust test_size for validation set to be 20% of the remaining data\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)  # Split for validation set\n",
        "\n",
        "    for train_index, val_index in sss.split(train_val_set, train_val_set['stratify_col_adjusted']):\n",
        "        train_set = train_val_set.iloc[train_index]\n",
        "        val_set = train_val_set.iloc[val_index]\n",
        "\n",
        "    # Save datasets based on target label\n",
        "    def save_datasets(dataset, set_name):\n",
        "        target_dir = os.path.join(output_dir, set_name)\n",
        "        os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "        # Save the entire dataset\n",
        "        dataset.to_csv(os.path.join(target_dir, f'{set_name}_set.csv'), index=False)\n",
        "\n",
        "    # Perform saving for train, validation, and test sets\n",
        "    save_datasets(train_set, 'train')\n",
        "    save_datasets(val_set, 'val')\n",
        "    save_datasets(test_set, 'test')\n",
        "\n",
        "    print(f\"Data split into train, validation, and test sets and saved to {output_dir}\")\n",
        "\n",
        "# Example usage\n",
        "# Assuming wavepattern_df is your DataFrame, 'Bay' is your target_label, and you have 'terrain' and 'tagger' as stratify columns\n",
        "# split_and_save_data(wavepattern_df, 'path/to/output', ['terrain', 'tagger'], 'Bay')\n"
      ],
      "metadata": {
        "id": "y06Zj-S9cm-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split_and_save_data(wavepattern_df, '/content/drive/MyDrive/Satalite_Surf_Project/wavepattern_model', ['terrain', 'tagger'],'Wave Pattern')\n",
        "# split_and_save_data(rivermouth_df, '/content/drive/MyDrive/Satalite_Surf_Project/rivermouth_model', ['terrain', 'tagger'],'Rivermouth')\n",
        "# split_and_save_data(bay_df, '/content/drive/MyDrive/Satalite_Surf_Project/bay_model', ['terrain', 'tagger'],'Bay')\n",
        "# split_and_save_data(other_df, '/content/drive/MyDrive/Satalite_Surf_Project/other_model', ['terrain', 'tagger'])\n",
        "# split_and_save_data(other_df, '/content/drive/MyDrive/Satalite_Surf_Project/combined_model', ['terrain', 'tagger'])\n",
        "# split_and_save_data(balanced_df, '/content/drive/MyDrive/Satalite_Surf_Project/alltogether_model', ['terrain', 'tagger'],'Good')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqNKq7lZcuJv",
        "outputId": "7531929b-76e5-45dc-971f-78bd3152b901",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stratify_col\n",
            "Unknown_Ori_0       34\n",
            "Unknown_Gilad_0     34\n",
            "Tropical_Alon_0     27\n",
            "Desert_Alon_1       25\n",
            "Unknown_Ori_1       24\n",
            "Unknown_Eyal_1      24\n",
            "Unknown_Gilad_1     20\n",
            "Tropical_Gilad_1    19\n",
            "Tropical_Alon_1     19\n",
            "Desert_Alon_0       18\n",
            "Tropical_Ron_0      15\n",
            "Unknown_Ron_0       13\n",
            "Tropical_Eyal_1     13\n",
            "Tropical_Ron_1      12\n",
            "Tropical_Gilad_0    12\n",
            "Tropical_Ori_0      12\n",
            "Unknown_Ron_1        8\n",
            "Tropical_Ori_1       5\n",
            "Unknown_Eyal_0       3\n",
            "Desert_Ron_0         1\n",
            "Desert_Gilad_1       1\n",
            "Tropical_Eyal_0      1\n",
            "Name: count, dtype: int64\n",
            "stratify_col_adjusted\n",
            "Unknown_Gilad_0     34\n",
            "Unknown_Ori_0       34\n",
            "Tropical_Alon_0     27\n",
            "Desert_Alon_1       25\n",
            "Unknown_Eyal_1      24\n",
            "Unknown_Ori_1       24\n",
            "Unknown_Gilad_1     20\n",
            "Tropical_Alon_1     19\n",
            "Tropical_Gilad_1    19\n",
            "Desert_Alon_0       18\n",
            "Tropical_Ron_0      15\n",
            "Tropical_Eyal_1     13\n",
            "Unknown_Ron_0       13\n",
            "Tropical_Ron_1      12\n",
            "Tropical_Gilad_0    12\n",
            "Tropical_Ori_0      12\n",
            "Unknown_Ron_1        8\n",
            "Other                6\n",
            "Tropical_Ori_1       5\n",
            "Name: count, dtype: int64\n",
            "Data split into train, validation, and test sets and saved to /content/drive/MyDrive/Satalite_Surf_Project/alltogether_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def populate_images(csv_path, source_dir, target_dir, target_label):\n",
        "    # Ensure the base target directory exists\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for target and non-target images\n",
        "    target_images_dir = os.path.join(target_dir, f'{target_label}')\n",
        "    non_target_images_dir = os.path.join(target_dir, f'not_{target_label}')\n",
        "    os.makedirs(target_images_dir, exist_ok=True)\n",
        "    os.makedirs(non_target_images_dir, exist_ok=True)\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Copy each image listed in the CSV to the appropriate target directory\n",
        "    for index, row in df.iterrows():\n",
        "        image_name = row['image_path']\n",
        "        label = row[target_label]  # Assuming the CSV contains a column with boolean or 1/0 indicating target label\n",
        "        source_path = os.path.join(source_dir, image_name)\n",
        "        target_path = os.path.join(target_images_dir if label == 1 else non_target_images_dir, image_name)\n",
        "\n",
        "        # Check if the source image exists before copying\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.copy(source_path, target_path)\n",
        "        else:\n",
        "            print(f\"Warning: {source_path} does not exist.\")\n",
        "\n",
        "def run_populate_images(model_name,target_label):\n",
        "# Directory paths\n",
        "  source_images_dir = '/content/images/Sat_Images'\n",
        "  train_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/train'\n",
        "  validation_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/val'\n",
        "  test_dir = f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/test'\n",
        "\n",
        "  # Populate images for each set\n",
        "  populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/train/train_set.csv', source_images_dir, train_dir,target_label)\n",
        "  populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/val/val_set.csv', source_images_dir, validation_dir,target_label)\n",
        "  populate_images(f'/content/drive/MyDrive/Satalite_Surf_Project/{model_name}/test/test_set.csv', source_images_dir, test_dir,target_label)\n",
        "\n",
        "# Directory paths\n",
        "# source_images_dir = '/content/images/Sat_Images'\n",
        "# train_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train/images'\n",
        "# validation_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val/images'\n",
        "# test_dir = '/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test/images'\n",
        "\n",
        "# # Populate images for each set\n",
        "# populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/train/train_set.csv', source_images_dir, train_dir)\n",
        "# populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/val/val_set.csv', source_images_dir, validation_dir)\n",
        "# populate_images('/content/drive/MyDrive/Satalite_Surf_Project/bay_model/test/test_set.csv', source_images_dir, test_dir)\n"
      ],
      "metadata": {
        "id": "4-DsIsmqfran"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip_path = '/content/drive/MyDrive/Satalite_Surf_Project/Sat_Images.zip'\n",
        "# unzipped_images_dir = '/content/images'\n",
        "# unzip_images(zip_path,unzipped_images_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBMaIwzSgPpT",
        "outputId": "2438996f-e99f-40e4-c04d-fd0501fe8e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images extracted to /content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run_populate_images('rivermouth_model','Rivermouth')\n",
        "# run_populate_images('wavepattern_model','Wave Pattern')\n",
        "# run_populate_images('bay_model','Bay')\n",
        "# run_populate_images('alltogether_model','Good')"
      ],
      "metadata": {
        "id": "297t-OLcaa92"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
